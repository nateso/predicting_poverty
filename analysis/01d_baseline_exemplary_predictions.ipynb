{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37007fa4",
   "metadata": {},
   "source": [
    "# Between model\n",
    "This model takes as input any variable that is static, that is the OSM variables, ESA Landcover variables and the WSF variables. Moreover, it takes the mean over all dynamic variables. The dynamic variables include Nightlights, NDVI, and NDWI_Gao as well as NDWI_McF. \n",
    "\n",
    "The idea is that the between model captures variation between clusters and thus the target variable for the between model is $\\bar{w}_c = \\frac{1}{T_c}\\sum_t^{T_c} w_{c,t}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948ff7f",
   "metadata": {},
   "source": [
    "# Within model\n",
    "This goal of this model is to predict the deviations from the cluster mean for each year. I.e. the model should capture variation within each cluster. To do so, the target variable is $\\tilde{w}_{ct} = w_{ct} - \\bar{w}_{c}$. \n",
    "\n",
    "For cluster $c$ in time period $t$, the feature vector is defined as $\\tilde{\\boldsymbol{x}}_{ct} = \\boldsymbol{x}_{ct} - \\bar{\\boldsymbol{x}}_{c}, where~\\bar{\\boldsymbol{x}}_{c} \\in \\mathbb{R}^{k\\times1}$. \n",
    "\n",
    "To predict $\\tilde{w}_{ct}$, I rely on $\\tilde{\\boldsymbol{x}}_{ct}$. This allows me to interpret the performance metric as the within R2, i.e. the share of the variance the model captures within clusters. \n",
    "\n",
    "\n",
    "(this does not help at all, thus disregard)...\n",
    "To augment the number of training observations, I train the model on deltas, rather than on the demeaned variables. This substantially increases the number of training observations and covers a wider range of differences, making the training dataset more versatile and robust. Ideally, this helps to learn from a wider range of differences and thus increases the out-of-sample when predicting $\\tilde{\\boldsymbol{w}}_{ct}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9ca298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8bc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the necessary functions from the analysis package\n",
    "\n",
    "# load the variable names, this allows to access the variables in the feature data in a compact way\n",
    "from analysis_utils.variable_names import *\n",
    "\n",
    "# load flagged ids \n",
    "from analysis_utils.flagged_uids import *\n",
    "\n",
    "# load the functions to do spatial k-fold CV\n",
    "from analysis_utils.spatial_CV import *\n",
    "\n",
    "# load the helper functions\n",
    "from analysis_utils.analysis_helpers import *\n",
    "\n",
    "# load the random forest trainer and cross_validator\n",
    "import analysis_utils.RandomForest as rf\n",
    "\n",
    "# load the combien model\n",
    "from analysis_utils.CombinedModel import CombinedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739dc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the global file paths\n",
    "root_data_dir = \"../../Data\"\n",
    "\n",
    "# the lsms data\n",
    "lsms_pth = f\"{root_data_dir}/lsms/processed/labels_cluster_v1.csv\"\n",
    "\n",
    "# the feature data\n",
    "feat_data_pth = f\"{root_data_dir}/feature_data/tabular_data.csv\"\n",
    "\n",
    "# set the random seed\n",
    "random_seed = 423\n",
    "spatial_cv_random_seed = 348\n",
    "\n",
    "# set the number of folds for k-fold CV\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c468b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations 6401\n",
      "Number of clusters 2128\n",
      "Number of x vars 113\n"
     ]
    }
   ],
   "source": [
    "# load the feature and the label data\n",
    "lsms_df = pd.read_csv(lsms_pth)\n",
    "# remove flagged ids form dataset\n",
    "lsms_df = lsms_df[~lsms_df.unique_id.isin(flagged_uids)].reset_index()\n",
    "lsms_df['avg_log_mean_pc_cons_usd_2017'] = lsms_df.groupby('cluster_id')['log_mean_pc_cons_usd_2017'].transform('mean')\n",
    "lsms_df['avg_mean_asset_index_yeh'] = lsms_df.groupby('cluster_id')['mean_asset_index_yeh'].transform('mean')\n",
    "feat_df = pd.read_csv(feat_data_pth)\n",
    "\n",
    "# describe the training data broadly\n",
    "print(f\"Number of observations {len(lsms_df)}\")\n",
    "print(f\"Number of clusters {len(np.unique(lsms_df.cluster_id))}\")\n",
    "print(f\"Number of x vars {len(feat_df.columns)-2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111de106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the label and the feature data to one dataset\n",
    "lsms_vars = ['unique_id', 'n_households',           \n",
    "             'log_mean_pc_cons_usd_2017', 'avg_log_mean_pc_cons_usd_2017',\n",
    "             'mean_asset_index_yeh', 'avg_mean_asset_index_yeh']\n",
    "df = pd.merge(lsms_df[lsms_vars], feat_df, on = 'unique_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24eb66",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec39c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the within and between x variables\n",
    "avg_rs_vars = avg_ndvi_vars + avg_ndwi_gao_vars + avg_nl_vars\n",
    "osm_vars = osm_dist_vars + osm_count_vars + osm_road_vars\n",
    "\n",
    "between_x_vars = osm_vars + esa_lc_vars + wsf_vars + avg_rs_vars + avg_preciptiation\n",
    "\n",
    "dyn_rs_vars = dyn_ndvi_vars + dyn_ndwi_gao_vars + dyn_nl_vars\n",
    "within_x_vars = dyn_rs_vars + precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbec1fc",
   "metadata": {},
   "source": [
    "### Target: Log per capita consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda1dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_target_var = 'avg_log_mean_pc_cons_usd_2017'\n",
    "cl_df = df[['cluster_id', between_target_var] + between_x_vars].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "# normalise the feature data\n",
    "cl_df_norm = standardise_df(cl_df, exclude_cols = [between_target_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "498e0205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'country', 'start_day', 'start_month', 'start_year', 'end_day',\n",
       "       'end_month', 'end_year', 'start_ts', 'end_ts', 'wave', 'series',\n",
       "       'cluster_id', 'rural', 'unique_id', 'lsms_lat', 'lsms_lon',\n",
       "       'mean_pc_cons_usd_2017', 'median_pc_cons_usd_2017',\n",
       "       'mean_pc_cons_lcu_2017', 'median_pc_cons_lcu_2017',\n",
       "       'mean_asset_index_nate', 'median_asset_index_nate',\n",
       "       'mean_asset_index_yeh', 'median_asset_index_yeh', 'n_households',\n",
       "       'extreme_poor', 'log_mean_pc_cons_usd_2017', 'country_series', 'lat',\n",
       "       'lon', 'avg_log_mean_pc_cons_usd_2017', 'avg_mean_asset_index_yeh'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsms_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f429484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the within dataframe\n",
    "# define the within variables\n",
    "within_target_var = 'loga'\n",
    "within_df = df[['cluster_id','unique_id', within_target_var] + within_x_vars]\n",
    "\n",
    "# demean the data and standardise the variables\n",
    "demeaned_df = demean_df(within_df)\n",
    "demeaned_df_norm = standardise_df(demeaned_df, exclude_cols = [within_target_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc87ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, specified test ratio: 0.2 - Actual test ratio 0.20\n",
      "Fold 1, specified test ratio: 0.2 - Actual test ratio 0.20\n",
      "Fold 2, specified test ratio: 0.2 - Actual test ratio 0.21\n",
      "Fold 3, specified test ratio: 0.2 - Actual test ratio 0.20\n",
      "Fold 4, specified test ratio: 0.2 - Actual test ratio 0.19\n",
      "Between training\n",
      "Initialising training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968635fcfcdd4ebba65f3f1906c544b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 170 seconds\n",
      "\n",
      "Within training\n",
      "Initialising training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b3bc589b744de382fd924bd88336ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 193 seconds\n"
     ]
    }
   ],
   "source": [
    "# divide the data into k different folds\n",
    "fold_ids = split_lsms_spatial(lsms_df, n_folds = n_folds, random_seed = spatial_cv_random_seed)\n",
    "\n",
    "# run the bewtween training\n",
    "print('Between training')\n",
    "between_cv_trainer_cons = rf.CrossValidator(cl_df_norm, \n",
    "                                            fold_ids, \n",
    "                                            between_target_var, \n",
    "                                            between_x_vars, \n",
    "                                            id_var = 'cluster_id', \n",
    "                                            random_seed = random_seed)\n",
    "between_cv_trainer_cons.run_cv_training(min_samples_leaf = 1)\n",
    "\n",
    "# run the within training\n",
    "print(\"\\nWithin training\")\n",
    "within_cv_trainer_cons = rf.CrossValidator(demeaned_df_norm, \n",
    "                                           fold_ids, \n",
    "                                           within_target_var, \n",
    "                                           within_x_vars, \n",
    "                                           id_var = 'unique_id', \n",
    "                                           random_seed = random_seed)\n",
    "within_cv_trainer_cons.run_cv_training(min_samples_leaf = 15)\n",
    "\n",
    "# combine both models\n",
    "combined_model_cons = CombinedModel(lsms_df, between_cv_trainer_cons, within_cv_trainer_cons)\n",
    "combined_model_cons.evaluate()\n",
    "combined_results = combined_model_cons.compute_overall_performance(use_fold_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49ff2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions\n",
    "combined_model_cons.pred_df.to_csv(\"results/baseline/exemplary_predictions.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
