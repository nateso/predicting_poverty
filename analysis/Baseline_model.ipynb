{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b9ca298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8bc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the necessary functions from the analysis package\n",
    "\n",
    "# load the variable names, this allows to access the variables in the feature data in a compact way\n",
    "from analysis_utils.variable_names import *\n",
    "\n",
    "# load the functions to do spatial k-fold CV\n",
    "from analysis_utils.spatial_CV import *\n",
    "\n",
    "# load the helper functions\n",
    "from analysis_utils.analysis_helpers import *\n",
    "\n",
    "# load the random forest trainer and cross_validator\n",
    "import analysis_utils.RandomForest as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739dc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the global file paths\n",
    "root_data_dir = \"../../Data\"\n",
    "\n",
    "lsms_pth = f\"{root_data_dir}/lsms/processed/labels_cluster_v1.csv\"\n",
    "\n",
    "feat_data_pth = f\"{root_data_dir}/feature_data/tabular_data.csv\"\n",
    "\n",
    "# set the random seed\n",
    "random_seed = 348\n",
    "\n",
    "# set the number of folds for k-fold CV\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c468b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations 7141\n",
      "Number of clusters 2255\n",
      "Number of x vars 63\n"
     ]
    }
   ],
   "source": [
    "# load the feature and the label data\n",
    "lsms_df = pd.read_csv(lsms_pth)\n",
    "lsms_df['avg_log_mean_pc_cons_usd_2017'] = lsms_df.groupby('cluster_id')['log_mean_pc_cons_usd_2017'].transform('mean')\n",
    "feat_df = pd.read_csv(feat_data_pth)\n",
    "\n",
    "# describe the training data broadly\n",
    "print(f\"Number of observations {len(lsms_df)}\")\n",
    "print(f\"Number of clusters {len(np.unique(lsms_df.cluster_id))}\")\n",
    "print(f\"Number of x vars {len(feat_df.columns)-2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4076385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, specified test ratio: 0.2 - Actual test ratio 0.21\n",
      "Fold 1, specified test ratio: 0.2 - Actual test ratio 0.21\n",
      "Fold 2, specified test ratio: 0.2 - Actual test ratio 0.20\n",
      "Fold 3, specified test ratio: 0.2 - Actual test ratio 0.20\n",
      "Fold 4, specified test ratio: 0.2 - Actual test ratio 0.18\n"
     ]
    }
   ],
   "source": [
    "# divide the data into k different folds\n",
    "fold_ids = split_lsms_spatial(lsms_df, n_folds = n_folds, random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111de106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the label and the feature data to one dataset\n",
    "df = pd.merge(lsms_df[['unique_id', 'log_mean_pc_cons_usd_2017', 'avg_log_mean_pc_cons_usd_2017','n_households']],\n",
    "             feat_df, on = 'unique_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24eb66",
   "metadata": {},
   "source": [
    "# Between model\n",
    "This model takes as input any variable that is static, that is the OSM variables, ESA Landcover variables and the WSF variables. Moreover, it takes the mean over all dynamic variables. The dynamic variables include Nightlights, NDVI, and NDWI_Gao as well as NDWI_McF. \n",
    "\n",
    "The idea is that the between model captures variation between clusters and thus the target variable for the between model is $\\bar{w}_c = \\frac{1}{T_c}\\sum_t^{T_c} w_{c,t}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda1dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dataset that only varies at the cluster level\n",
    "between_x_vars = osm_dist_pca_vars + osm_count_pca_vars + esa_lc_vars + wsf_vars + rs_mean_vars + ['avg_precipitation']\n",
    "between_target_var = 'avg_log_mean_pc_cons_usd_2017'\n",
    "cl_df = df[['cluster_id', between_target_var] + between_x_vars].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "# normalise the feature data\n",
    "cl_df_norm = standardise_df(cl_df, exclude_cols = [between_target_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc87ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2359775129904a80b9ca08f989376d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 121 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'r2': 0.4638394412893702, 'mse': 0.15337512765665054}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the bewtween training\n",
    "between_cv_trainer = rf.CrossValidator(cl_df_norm, fold_ids, between_target_var, between_x_vars, id_var = 'cluster_id', random_seed = random_seed)\n",
    "between_cv_trainer.run_cv_training()\n",
    "between_cv_trainer.compute_overall_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2752d",
   "metadata": {},
   "source": [
    "# Within model\n",
    "This goal of this model is to predict the deviations from the cluster mean for each year. I.e. the model should capture variation within each cluster. To do so, the target variable is $\\tilde{w}_{ct} = w_{ct} - \\bar{w}_{ct}$. \n",
    "\n",
    "For cluster $c$ in time period $t$, the feature vector is defined as $\\tilde{\\boldsymbol{x}}_{ct} = \\boldsymbol{x}_{ct} - \\bar{\\boldsymbol{x}}_{ct}, where~\\bar{\\boldsymbol{x}}_{ct} \\in \\mathbb{R}^{k\\times1}$. \n",
    "\n",
    "To predict $\\tilde{w}_{ct}$, I rely on $\\tilde{\\boldsymbol{x}}_{ct}$. This allows me to interpret the performance metric as the within R2, i.e. the share of the variance the model captures within clusters. \n",
    "\n",
    "\n",
    "(this does not help at all, thus disregard)...\n",
    "To augment the number of training observations, I train the model on deltas, rather than on the demeaned variables. This substantially increases the number of training observations and covers a wider range of differences, making the training dataset more versatile and robust. Ideally, this helps to learn from a wider range of differences and thus increases the out-of-sample when predicting $\\tilde{\\boldsymbol{w}}_{ct}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f41a8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the within variables\n",
    "within_x_vars = rs_dyn_vars + ['precipitation']\n",
    "within_target_var = 'log_mean_pc_cons_usd_2017'\n",
    "within_df = df[['cluster_id','unique_id', within_target_var] + within_x_vars]\n",
    "\n",
    "# demean the data and standardise the variables\n",
    "demeaned_df = demean(within_df)\n",
    "demeaned_df_norm = standardise_df(demeaned_df, exclude_cols = [within_target_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4554c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522c964f68ba4e72900723f7d8444d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 295 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'r2': 0.010477796846323209, 'mse': 0.05445850180218004}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the within training\n",
    "within_cv_trainer = rf.CrossValidator(demeaned_df_norm, fold_ids, within_target_var, within_x_vars, id_var = 'unique_id', random_seed = random_seed)\n",
    "within_cv_trainer.run_cv_training(min_samples_leaf = 15)\n",
    "within_cv_trainer.compute_overall_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eab12e",
   "metadata": {},
   "source": [
    "# Overall model\n",
    "This model combines the within and between model to make the overall predictions on the validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d3765f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the target variable\n",
    "target_var = 'log_mean_pc_cons_usd_2017'\n",
    "\n",
    "# merge the predictions to one dataset\n",
    "preds = pd.DataFrame(within_cv_trainer.predictions)\n",
    "preds = pd.merge(preds, df[['cluster_id','unique_id']], on = 'unique_id')\n",
    "preds = pd.merge(preds[['unique_id', 'cluster_id', 'y_hat']],\n",
    "                 pd.DataFrame(between_cv_trainer.predictions)[['cluster_id', 'y_hat']],\n",
    "                 on = 'cluster_id', how = 'left', suffixes = ('_change', '_mn'))\n",
    "preds['y_hat'] = preds['y_hat_change'] + preds['y_hat_mn']\n",
    "\n",
    "# get the ground truth value\n",
    "preds = pd.merge(preds, df[['unique_id', target_var]], on = 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "187393d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the final predictions based on the k-fold CV workflow\n",
    "r2 = []\n",
    "for fold, splits in fold_ids.items():\n",
    "    # get the training and validation sample\n",
    "    train_df, val_df = split_lsms_ids(preds, splits['val_ids'])\n",
    "    y_hat_val = val_df['y_hat']\n",
    "    y_val = val_df[target_var]\n",
    "    \n",
    "    # calculate the performance on the validation sample\n",
    "    r2.append(r2_score(y_val, y_hat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d515878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3976245560985697"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_weights = [len(v['val_ids'])/(len(v['val_ids']) + len(v['train_ids'])) for v in fold_ids.values()]\n",
    "np.average(r2, weights = fold_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0baee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to plot the data\n",
    "preds.to_csv(\"../results/predictions/baseline_preds.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357aa3dc",
   "metadata": {},
   "source": [
    "# Delta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the within variables\n",
    "within_x_vars = rs_dyn_vars + ['precipitation']\n",
    "within_target_var = 'log_mean_pc_cons_usd_2017'\n",
    "within_df = df[['cluster_id','unique_id', within_target_var] + within_x_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02652d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "demeaned_df = demean(within_df)\n",
    "delta_df = make_delta_df(within_df)\n",
    "\n",
    "# combine the delta df, with the demeaned df\n",
    "demeaned_df = demeaned_df.rename(columns = {'unique_id': 'delta_id'})\n",
    "delta_df = pd.concat([delta_df, demeaned_df]).reset_index(drop = True)\n",
    "delta_df_norm = standardise_df(delta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the within training\n",
    "within_cv_trainer = rf.CrossValidator(delta_df_norm, fold_ids, within_target_var, within_x_vars, id_var = 'delta_id', random_seed = random_seed)\n",
    "within_cv_trainer.run_cv_training(min_samples_leaf = 15)\n",
    "within_cv_trainer.compute_overall_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the demeaned df\n",
    "within_evaluator = rf.CV_Evaluator(standardise_df(demeaned_df), fold_ids, within_cv_trainer, id_var = 'delta_id')\n",
    "within_evaluator.evaluate()\n",
    "within_evaluator.compute_overall_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b8d77",
   "metadata": {},
   "source": [
    "# Calculate for different number of households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb774b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_households = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "\n",
    "between_x_vars = osm_dist_pca_vars + osm_count_pca_vars + esa_lc_vars + wsf_vars + rs_mean_vars + ['avg_precipitation']\n",
    "between_target_var = 'avg_log_mean_pc_cons_usd_2017'\n",
    "between_df = df[['cluster_id','n_households', between_target_var] + between_x_vars].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "within_x_vars = rs_dyn_vars + ['precipitation']\n",
    "within_target_var = 'log_mean_pc_cons_usd_2017'\n",
    "within_df = df[['cluster_id','n_households','unique_id', within_target_var] + within_x_vars]\n",
    "\n",
    "between_r2 = []\n",
    "within_r2 = []\n",
    "\n",
    "for i in n_households:\n",
    "    print(f\"Training on at least {i} Households per cluster\")\n",
    "    between_df_sub = between_df[between_df['n_households'] >= i].drop(columns = 'n_households').reset_index(drop = True)\n",
    "    within_df_sub = within_df[within_df['n_households'] >= i].drop(columns = 'n_households').reset_index(drop = True)\n",
    "    \n",
    "    between_df_norm = standardise_df(between_df_sub, exclude_cols = [between_target_var])\n",
    "    demeaned_df_norm = standardise_df(demean(within_df_sub), exclude_cols = [within_target_var])\n",
    "\n",
    "    # run the bewtween training\n",
    "    print('\\nBetween model')\n",
    "    between_cv_trainer = rf.CrossValidator(between_df_norm, fold_ids, between_target_var, between_x_vars, id_var = 'cluster_id', random_seed = random_seed)\n",
    "    between_cv_trainer.run_cv_training(min_samples_leaf = 5)\n",
    "    between_res = between_cv_trainer.compute_overall_performance()\n",
    "    print(between_res)\n",
    "    \n",
    "    # run the within training\n",
    "    print('\\nWithin Model')\n",
    "    within_cv_trainer = rf.CrossValidator(demeaned_df_norm, fold_ids, within_target_var, within_x_vars, id_var = 'unique_id', random_seed = random_seed)\n",
    "    within_cv_trainer.run_cv_training(min_samples_leaf = 15)\n",
    "    within_res = within_cv_trainer.compute_overall_performance()\n",
    "    print(within_res)\n",
    "    \n",
    "    # store results\n",
    "    between_r2.append(between_res['r2'])\n",
    "    within_r2.append(within_res['r2'])\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4,4))\n",
    "plt.plot(list(range(1,16)), between_r2, label = 'Between $R^2$')\n",
    "plt.plot(list(range(1,16)), within_r2, label = 'Within $R^2$')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Minium number of housheolds per cluster\")\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.axhline(y=0, color='red', linestyle='dotted', label='y = 0')  # Add red dotted line at y = 0\n",
    "plt.xticks(range(0, 16))  # Set x-axis ticks from 1 to 10\n",
    "plt.savefig(\"../figures/results/R2_vs_households.png\", dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
